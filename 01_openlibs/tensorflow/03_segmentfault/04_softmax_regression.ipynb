{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow学习笔记（4）：基于MNIST数据的softmax regression\n",
    "https://segmentfault.com/a/1190000008110394"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前言\n",
    "本文基于TensorFlow官网的Tutorial写成。输入数据是MNIST，全称是Modified National Institute of Standards and Technology，是一组由这个机构搜集的手写数字扫描文件和每个文件对应标签的数据集，经过一定的修改使其适合机器学习算法读取。这个数据集可以从牛的不行的Yann LeCun教授的网站获取。\n",
    "\n",
    "本文首先使用sklearn的LogisticRegression()进行训练，得到的参数绘制效果如下（红色表示参数估计结果为负，蓝色表示参数估计结果为正，绿色代表参数估计结果为零）：\n",
    "\n",
    "![softmax](https://sfault-image.b0.upaiyun.com/350/671/350671353-5878750d7b201_articlex)\n",
    "\n",
    "从图形效果看，我们发现蓝色点组成的轮廓与对应的数字轮廓还是比较接近的。\n",
    "\n",
    "然后本文使用tensorflow对同样的数据集进行了softmax regression的训练，得到的参数绘制效果如下：\n",
    "\n",
    "![softmax](https://sfault-image.b0.upaiyun.com/183/296/1832960746-587875aae1d1a_articlex)\n",
    "\n",
    "蓝色点组成的轮廓与对应的数字轮廓比较接近。但是对比上下两幅截图，感觉tensorflow的效果更平滑一些。不过从测试集的准确率来看，二者都在92%左右，sklearn稍微好一点。注意，92%的准确率看起来不错，但其实是一个很低的准确率，按照官网教程的说法，应该要感到羞愧。\n",
    "\n",
    "## 代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing MNIST handwritten digits data...\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import struct\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def read_image(file_name):\n",
    "    with gzip.open(file_name, 'rb') as f:\n",
    "        buf = f.read()\n",
    "        index = 0\n",
    "        magic, images, rows, columns = struct.unpack_from('>IIII', buf, index)\n",
    "        index += struct.calcsize('>IIII')\n",
    "        \n",
    "        image_size = '>' + str(images * rows * columns) + 'B'\n",
    "        ims = struct.unpack_from(image_size, buf, index)\n",
    "        \n",
    "        im_array = np.array(ims).reshape(images, rows, columns)\n",
    "        return im_array\n",
    "    \n",
    "def read_label(file_name):\n",
    "    with gzip.open(file_name, 'rb') as f:\n",
    "        buf = f.read()\n",
    "        index = 0\n",
    "        magic, labels = struct.unpack_from('>II', buf, index)\n",
    "        index += struct.calcsize('>II')\n",
    "        \n",
    "        label_size = '>' + str(labels) + 'B'\n",
    "        labels = struct.unpack_from(label_size, buf, index)\n",
    "        \n",
    "        label_array = np.array(labels)\n",
    "        return label_array\n",
    "    \n",
    "print(\"Start processing MNIST handwritten digits data...\")\n",
    "train_x_data = read_image(\"MNIST_data/train-images-idx3-ubyte.gz\")\n",
    "train_x_data = train_x_data.reshape(train_x_data.shape[0], -1).astype(np.float32)\n",
    "train_y_data = read_label(\"MNIST_data/train-labels-idx1-ubyte.gz\")\n",
    "test_x_data = read_image(\"MNIST_data/t10k-images-idx3-ubyte.gz\")\n",
    "test_x_data = test_x_data.reshape(test_x_data.shape[0], -1).astype(np.float32)\n",
    "test_y_data = read_label(\"MNIST_data/t10k-labels-idx1-ubyte.gz\")\n",
    "\n",
    "train_x_minmax = train_x_data / 255\n",
    "test_x_minmax = test_x_data / 255\n",
    "\n",
    "# Of course you can also use the utility function to read in MNIST provided by tensorflow\n",
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "# mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=False)\n",
    "# train_x_minmax = mnist.train.images\n",
    "# train_y_data = mnist.train.labels\n",
    "# test_x_minmax = mnist.test.images\n",
    "# test_y_data = mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start evaluating softmax regression model by sklearn\n",
      "Accuracy of test set: 0.926300\n"
     ]
    }
   ],
   "source": [
    "# We evaluate the softmax regression model by sklearn first\n",
    "eval_sklearn = True\n",
    "if eval_sklearn:\n",
    "    print(\"Start evaluating softmax regression model by sklearn\")\n",
    "    reg = LogisticRegression(solver=\"lbfgs\", multi_class=\"multinomial\")\n",
    "    reg.fit(train_x_minmax, train_y_data)\n",
    "    # Save coefficients to a text file\n",
    "    np.savetxt(\"output/coef_softmax_sklearn.txt\", reg.coef_, fmt=\"%.6f\")\n",
    "    test_y_predict = reg.predict(test_x_minmax)\n",
    "    print(\"Accuracy of test set: %f\" % accuracy_score(test_y_data, test_y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start evaluating softmax regression model by tensorflow...\n",
      "Stochastic Gradient Descent processing step 0\n",
      "Stochastic Gradient Descent processing step 100\n",
      "Stochastic Gradient Descent processing step 200\n",
      "Stochastic Gradient Descent processing step 300\n",
      "Stochastic Gradient Descent processing step 400\n",
      "Stochastic Gradient Descent processing step 500\n",
      "Stochastic Gradient Descent processing step 600\n",
      "Stochastic Gradient Descent processing step 700\n",
      "Stochastic Gradient Descent processing step 800\n",
      "Stochastic Gradient Descent processing step 900\n",
      "Accuracy of test set: 0.916700\n"
     ]
    }
   ],
   "source": [
    "eval_tensorflow = True\n",
    "batch_gradient = False\n",
    "if eval_tensorflow:\n",
    "    print(\"Start evaluating softmax regression model by tensorflow...\")\n",
    "    # reformat y into one-hot encoding style\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(train_y_data)\n",
    "    train_y_data_trans = lb.transform(train_y_data)\n",
    "    test_y_data_trans = lb.transform(test_y_data)\n",
    "\n",
    "    x = tf.placeholder(tf.float32, [None, 784])\n",
    "    W = tf.Variable(tf.zeros([784, 10]))\n",
    "    b = tf.Variable(tf.zeros([10]))\n",
    "    V = tf.matmul(x, W) + b\n",
    "    y = tf.nn.softmax(V)\n",
    "\n",
    "    y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "    loss = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "    train = optimizer.minimize(loss)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "\n",
    "    if batch_gradient:\n",
    "        for step in range(300):\n",
    "            sess.run(train, feed_dict={x: train_x_minmax, y_: train_y_data_trans})\n",
    "            if step % 10 == 0:\n",
    "                print(\"Batch Gradient Descent processing step %d\" % step)\n",
    "        print(\"Finally we got the estimated results, take such a long time...\")\n",
    "    else:\n",
    "        for step in range(1000):\n",
    "            sample_index = np.random.choice(train_x_minmax.shape[0], 100)\n",
    "            batch_xs = train_x_minmax[sample_index, :]\n",
    "            batch_ys = train_y_data_trans[sample_index, :]\n",
    "            sess.run(train, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "            if step % 100 == 0:\n",
    "                print(\"Stochastic Gradient Descent processing step %d\" % step)\n",
    "    # Save coefficients to a text file\n",
    "    np.savetxt('output/coef_softmax_tf.txt', np.transpose(sess.run(W)), fmt='%.6f')\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy of test set: %f\"\n",
    "          % sess.run(accuracy, feed_dict={x: test_x_minmax, \n",
    "                                          y_: test_y_data_trans}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 思考\n",
    "\n",
    "sklearn的估计时间有点长，因为每一轮参数更新都是基于全量的训练集数据算出损失，再算出梯度，然后再改进结果的。\n",
    "\n",
    "tensorflow采用batch gradient descent估计算法时，时间也比较长，原因同上。\n",
    "\n",
    "tensorflow采用stochastic gradient descent估计算法时间短，最后的估计结果也挺好，相当于每轮迭代只用到了部分数据集算出损失和梯度，速度变快，但可能bias增加；所以把迭代次数增多，这样可以降低variance，总体上的误差相比batch gradient descent并没有差多少。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
