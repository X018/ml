{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow学习笔记（3）：逻辑回归\n",
    "https://segmentfault.com/a/1190000008011596"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前言\n",
    "本文使用tensorflow训练逻辑回归模型，并将其与scikit-learn做比较。数据集来自Andrew Ng的网上公开课程[Deep Learning](http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=DeepLearning&doc=exercises/ex4/ex4.html)\n",
    "\n",
    "## 代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-29caff1eb118>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "\n",
    "x_data = np.loadtxt(\"input/ex4x.dat\").astype(np.float32)\n",
    "y_data = np.loadtxt(\"input/ex4y.dat\").astype(np.float32)\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(x_data)\n",
    "x_data_standard = scaler.transform(x_data)\n",
    "\n",
    "# Set C as a large positive number to minimize the regularization effect\n",
    "reg = LogisticRegression(C=999999999, solver=\"newton-cg\")\n",
    "reg.fit(x_data, y_data)\n",
    "print(\"Coefficients of sklearn: K=%s, b=%f\" % (reg.coef_, reg.intercept_))\n",
    "\n",
    "print(\"-----------------------------------------------------------------\")\n",
    "\n",
    "# Now we use tensorflow to get similar results\n",
    "W = tf.Variable(tf.zeros([2, 1]))\n",
    "b = tf.Variable(tf.zeros([1, 1]))\n",
    "y = 1 / (1 + tf.exp(-tf.matmul(x_data_standard, W) + b))\n",
    "loss = tf.reduce_mean(- y_data.reshape(-1, 1) * tf.log(y)\n",
    "                      - (1 - y_data.reshape(-1, 1)) * tf.log(1 - y))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(1.3)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(100):\n",
    "    sess.run(train)\n",
    "    if step % 10 == 0:\n",
    "        print(step, sess.run(W).flatten(), sess.run(b).flatten())\n",
    "        \n",
    "        \n",
    "print(\"Coefficients of tensorflow (input should be standardized): K=%s, b=%s\" \n",
    "      % (sess.run(W).flatten(), sess.run(b).flatten()))\n",
    "print(\"Coefficients of tensorflow (raw input): K=%s, b=%s\"\n",
    "      % (sess.run(W).flatten() / scaler.scale_, \n",
    "         sess.run(b).flatten() - np.dot(scaler.mean_ / scaler.scale_, \n",
    "                                        sess.run(W))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem solved and we are happy. But...\n",
    "\n",
    "I'd like to implement the logistic regression from a multi-class viewpoint instead of binary.\n",
    "\n",
    "In machine learning domain, it is called softmax regression\n",
    "\n",
    "In economic and statistics domain, it is called multinomial logit (MNL) model, proposed by Daniel McFadden, who shared the 2000  Nobel Memorial Prize in Economic Sciences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg = LogisticRegression(C=999999999, solver=\"newton-cg\", \n",
    "                         multi_class=\"multinomial\")\n",
    "reg.fit(x_data, y_data)\n",
    "print(\"Coefficients of sklearn: K=%s, b=%f\"\n",
    "     % (reg.coef_, reg.intercept_))\n",
    "print(\"A little bit difference at first glance. What about multipy tem with 2\")\n",
    "\n",
    "# Try tensorflow\n",
    "# first 2 is feature number, second 2 is class number\n",
    "W = tf.Variable(tf.zeros([2, 2]))\n",
    "b = tf.Variable(tf.zeros([1, 2]))\n",
    "V = tf.matmul(x_data_standard, W) + b\n",
    "\n",
    "# tensorflow provide a utility function to calculate the probability \n",
    "# of observer n choose alternative i, you can replace it with \n",
    "# `y = tf.exp(V) / tf.reduce_sum(tf.exp(V), keep_dims=True, \n",
    "# reduction_indices=[1])`\n",
    "y = tf.nn.softmax(V)\n",
    "\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(y_data)\n",
    "\n",
    "y_data_trans = lb.transform(y_data)\n",
    "# Only necessary for binary class \n",
    "y_data_trans = np.concatenate((1 - y_data_trans, y_data_trans), axis=1)\n",
    "\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(y_data_trans * tf.log(y), \n",
    "                                     reduction_indices=[1]))\n",
    "optimizer = tf.train.GradientDescentOptimizer(1.3)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "for step in range(100):\n",
    "    sess.run(train)\n",
    "    if step % 10 == 0:\n",
    "        print(step, sess.run(W).flatten(), sess.run(b).flatten())\n",
    "        \n",
    "    print(\"Coefficients of tensorflow (input should be standardized): K=%s, b=%s\" \n",
    "          % (sess.run(W).flatten(), sess.run(b).flatten()))\n",
    "    print(\"Coefficients of tensorflow (raw input): K=%s, b=%s\" \n",
    "          % ((sess.run(W) / scaler.scale_).flatten(), \n",
    "             sess.run(b).flatten() - np.dot(scaler.mean_ / scaler.scale_, \n",
    "                                            sess.run(W))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
